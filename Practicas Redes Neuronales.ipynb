{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "dataset=np.loadtxt('D:/Ivan/Tesis/EigenfacesPython/3.2.+Dise√±oRedNeuronal/Datasets/pima-indians-diabetes.csv',delimiter=',')\n",
    "print(dataset.shape)\n",
    "X=dataset[:,0:8]\n",
    "y=dataset[:,8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Definir el modelo\n",
    "model= Sequential()\n",
    "model.add(Dense(12,input_dim=8,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "48/48 [==============================] - 1s 3ms/step - loss: 4.7436 - accuracy: 0.5586\n",
      "Epoch 2/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 1.9005 - accuracy: 0.6263\n",
      "Epoch 3/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 1.2281 - accuracy: 0.6354\n",
      "Epoch 4/150\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.9442 - accuracy: 0.6263\n",
      "Epoch 5/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.8412 - accuracy: 0.6250\n",
      "Epoch 6/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.7973 - accuracy: 0.6198\n",
      "Epoch 7/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.7758 - accuracy: 0.6341\n",
      "Epoch 8/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.7648 - accuracy: 0.6289\n",
      "Epoch 9/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.7494 - accuracy: 0.6393\n",
      "Epoch 10/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.7427 - accuracy: 0.6523\n",
      "Epoch 11/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.7294 - accuracy: 0.6523\n",
      "Epoch 12/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.7133 - accuracy: 0.6536\n",
      "Epoch 13/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.7170 - accuracy: 0.6536\n",
      "Epoch 14/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.7005 - accuracy: 0.6549\n",
      "Epoch 15/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6936 - accuracy: 0.6615\n",
      "Epoch 16/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.6589\n",
      "Epoch 17/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6810 - accuracy: 0.6562\n",
      "Epoch 18/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6758 - accuracy: 0.6628\n",
      "Epoch 19/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6686 - accuracy: 0.6589\n",
      "Epoch 20/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6629 - accuracy: 0.6641\n",
      "Epoch 21/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6630 - accuracy: 0.6602\n",
      "Epoch 22/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6603 - accuracy: 0.6641\n",
      "Epoch 23/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6569 - accuracy: 0.6641\n",
      "Epoch 24/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6511 - accuracy: 0.6615\n",
      "Epoch 25/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6584 - accuracy: 0.6549\n",
      "Epoch 26/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6487 - accuracy: 0.6641\n",
      "Epoch 27/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6491 - accuracy: 0.6654\n",
      "Epoch 28/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6413 - accuracy: 0.6667\n",
      "Epoch 29/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6443 - accuracy: 0.6589\n",
      "Epoch 30/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6382 - accuracy: 0.6654\n",
      "Epoch 31/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6435 - accuracy: 0.6641\n",
      "Epoch 32/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6453 - accuracy: 0.6615\n",
      "Epoch 33/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6383 - accuracy: 0.6667\n",
      "Epoch 34/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6351 - accuracy: 0.6641\n",
      "Epoch 35/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6341 - accuracy: 0.6680\n",
      "Epoch 36/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6400 - accuracy: 0.6602\n",
      "Epoch 37/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6332 - accuracy: 0.6654\n",
      "Epoch 38/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6333 - accuracy: 0.6628\n",
      "Epoch 39/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6289 - accuracy: 0.6641\n",
      "Epoch 40/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6287 - accuracy: 0.6680\n",
      "Epoch 41/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6267 - accuracy: 0.6693\n",
      "Epoch 42/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6304 - accuracy: 0.6654\n",
      "Epoch 43/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6311 - accuracy: 0.6628\n",
      "Epoch 44/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6288 - accuracy: 0.6628\n",
      "Epoch 45/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6233 - accuracy: 0.6680\n",
      "Epoch 46/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6251 - accuracy: 0.6615\n",
      "Epoch 47/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6236 - accuracy: 0.6680\n",
      "Epoch 48/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6221 - accuracy: 0.6641\n",
      "Epoch 49/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6231 - accuracy: 0.6654\n",
      "Epoch 50/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6228 - accuracy: 0.6667\n",
      "Epoch 51/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6291 - accuracy: 0.6628\n",
      "Epoch 52/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6298 - accuracy: 0.6667\n",
      "Epoch 53/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6230 - accuracy: 0.6667\n",
      "Epoch 54/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6230 - accuracy: 0.6654\n",
      "Epoch 55/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6204 - accuracy: 0.6628\n",
      "Epoch 56/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6236 - accuracy: 0.6654\n",
      "Epoch 57/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6180 - accuracy: 0.6667\n",
      "Epoch 58/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6142 - accuracy: 0.6654\n",
      "Epoch 59/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6143 - accuracy: 0.6667\n",
      "Epoch 60/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6209 - accuracy: 0.6615\n",
      "Epoch 61/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6190 - accuracy: 0.6667\n",
      "Epoch 62/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6161 - accuracy: 0.6680\n",
      "Epoch 63/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6164 - accuracy: 0.6654\n",
      "Epoch 64/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6139 - accuracy: 0.6706\n",
      "Epoch 65/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6174 - accuracy: 0.6628\n",
      "Epoch 66/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6115 - accuracy: 0.6693\n",
      "Epoch 67/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6176 - accuracy: 0.6667\n",
      "Epoch 68/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6137 - accuracy: 0.6719\n",
      "Epoch 69/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6139 - accuracy: 0.6667\n",
      "Epoch 70/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6135 - accuracy: 0.6693\n",
      "Epoch 71/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6098 - accuracy: 0.6680\n",
      "Epoch 72/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6162 - accuracy: 0.6680\n",
      "Epoch 73/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6126 - accuracy: 0.6719\n",
      "Epoch 74/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6175 - accuracy: 0.6654\n",
      "Epoch 75/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6127 - accuracy: 0.6680\n",
      "Epoch 76/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6168 - accuracy: 0.6667\n",
      "Epoch 77/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6098 - accuracy: 0.6706\n",
      "Epoch 78/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6122 - accuracy: 0.6667\n",
      "Epoch 79/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6132 - accuracy: 0.6732\n",
      "Epoch 80/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6159 - accuracy: 0.6680\n",
      "Epoch 81/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6070 - accuracy: 0.6719\n",
      "Epoch 82/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6158 - accuracy: 0.6693\n",
      "Epoch 83/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6109 - accuracy: 0.6667\n",
      "Epoch 84/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6106 - accuracy: 0.6680\n",
      "Epoch 85/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6090 - accuracy: 0.6654\n",
      "Epoch 86/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6134 - accuracy: 0.6680\n",
      "Epoch 87/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6100 - accuracy: 0.6693\n",
      "Epoch 88/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6051 - accuracy: 0.6654\n",
      "Epoch 89/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6077 - accuracy: 0.6680\n",
      "Epoch 90/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6105 - accuracy: 0.6693\n",
      "Epoch 91/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6097 - accuracy: 0.6654\n",
      "Epoch 92/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6156 - accuracy: 0.6706\n",
      "Epoch 93/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6095 - accuracy: 0.6680\n",
      "Epoch 94/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6058 - accuracy: 0.6732\n",
      "Epoch 95/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6151 - accuracy: 0.6654\n",
      "Epoch 96/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6081 - accuracy: 0.6680\n",
      "Epoch 97/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6024 - accuracy: 0.6706\n",
      "Epoch 98/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6019 - accuracy: 0.6719\n",
      "Epoch 99/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6032 - accuracy: 0.6719\n",
      "Epoch 100/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6054 - accuracy: 0.6719\n",
      "Epoch 101/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5991 - accuracy: 0.6732\n",
      "Epoch 102/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6000 - accuracy: 0.6719\n",
      "Epoch 103/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5991 - accuracy: 0.6745\n",
      "Epoch 104/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6035 - accuracy: 0.6667\n",
      "Epoch 105/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6074 - accuracy: 0.6654\n",
      "Epoch 106/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6002 - accuracy: 0.6706\n",
      "Epoch 107/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5960 - accuracy: 0.6745\n",
      "Epoch 108/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5974 - accuracy: 0.6719\n",
      "Epoch 109/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5974 - accuracy: 0.6732\n",
      "Epoch 110/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5969 - accuracy: 0.6693\n",
      "Epoch 111/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5992 - accuracy: 0.6706\n",
      "Epoch 112/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5934 - accuracy: 0.6719\n",
      "Epoch 113/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5971 - accuracy: 0.6771\n",
      "Epoch 114/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5922 - accuracy: 0.6745\n",
      "Epoch 115/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5917 - accuracy: 0.6758\n",
      "Epoch 116/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5940 - accuracy: 0.6680\n",
      "Epoch 117/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5965 - accuracy: 0.6706\n",
      "Epoch 118/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.6706\n",
      "Epoch 119/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5948 - accuracy: 0.6667\n",
      "Epoch 120/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.6096 - accuracy: 0.6667\n",
      "Epoch 121/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6719\n",
      "Epoch 122/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5923 - accuracy: 0.6745\n",
      "Epoch 123/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5933 - accuracy: 0.6732\n",
      "Epoch 124/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5900 - accuracy: 0.6719\n",
      "Epoch 125/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5900 - accuracy: 0.6719\n",
      "Epoch 126/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5869 - accuracy: 0.6706\n",
      "Epoch 127/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5856 - accuracy: 0.6719\n",
      "Epoch 128/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.6758\n",
      "Epoch 129/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5957 - accuracy: 0.6680\n",
      "Epoch 130/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5833 - accuracy: 0.6758\n",
      "Epoch 131/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.6706\n",
      "Epoch 132/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5814 - accuracy: 0.6719\n",
      "Epoch 133/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5796 - accuracy: 0.6758\n",
      "Epoch 134/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5819 - accuracy: 0.6706\n",
      "Epoch 135/150\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.5853 - accuracy: 0.6719\n",
      "Epoch 136/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5775 - accuracy: 0.6745\n",
      "Epoch 137/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5759 - accuracy: 0.6745\n",
      "Epoch 138/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5763 - accuracy: 0.6719\n",
      "Epoch 139/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5782 - accuracy: 0.6719\n",
      "Epoch 140/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5763 - accuracy: 0.6797\n",
      "Epoch 141/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5775 - accuracy: 0.6745\n",
      "Epoch 142/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5763 - accuracy: 0.6719\n",
      "Epoch 143/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5707 - accuracy: 0.6992\n",
      "Epoch 144/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5664 - accuracy: 0.7109\n",
      "Epoch 145/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5656 - accuracy: 0.7031\n",
      "Epoch 146/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5581 - accuracy: 0.7109\n",
      "Epoch 147/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5591 - accuracy: 0.7096\n",
      "Epoch 148/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5574 - accuracy: 0.7188\n",
      "Epoch 149/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5637 - accuracy: 0.7109\n",
      "Epoch 150/150\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5546 - accuracy: 0.7148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2573f27ec20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajuste del modelo\n",
    "model.fit(X,y,epochs=150,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 0.5477 - accuracy: 0.7122\n",
      "Accuracy:71.22\n"
     ]
    }
   ],
   "source": [
    "# Evaluaci√≥n del Modelo\n",
    "_, accuracy=model.evaluate(X,y)\n",
    "print('Accuracy:%.2f'% (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicciones\n",
    "predictions = (model.predict(X) > 0.5).astype(\"int32\")\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 627.0, 50.0] --> 1 (real 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 351.0, 31.0] --> 0 (real 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 672.0, 32.0] --> 1 (real 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 167.0, 21.0] --> 0 (real 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2288.0, 33.0] --> 1 (real 1)\n",
      "[5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 201.0, 30.0] --> 0 (real 0)\n",
      "[3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 248.0, 26.0] --> 0 (real 1)\n",
      "[10.0, 115.0, 0.0, 0.0, 0.0, 35.3, 134.0, 29.0] --> 0 (real 0)\n",
      "[2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 158.0, 53.0] --> 1 (real 1)\n",
      "[8.0, 125.0, 96.0, 0.0, 0.0, 0.0, 232.0, 54.0] --> 0 (real 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BBY2021\\AppData\\Local\\Temp\\ipykernel_7228\\97878265.py:2: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print('%s --> %d (real %d)'% (X[i].tolist(),predictions[i],y[i]))\n"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "    print('%s --> %d (real %d)'% (X[i].tolist(),predictions[i],y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "Epoch 1/150\n",
      "33/33 [==============================] - 1s 12ms/step - loss: 83.2192 - accuracy: 0.3599 - val_loss: 64.2358 - val_accuracy: 0.3268\n",
      "Epoch 2/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 53.7706 - accuracy: 0.3599 - val_loss: 39.7666 - val_accuracy: 0.3307\n",
      "Epoch 3/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 32.3329 - accuracy: 0.3696 - val_loss: 22.1836 - val_accuracy: 0.3701\n",
      "Epoch 4/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 17.4315 - accuracy: 0.4008 - val_loss: 11.2531 - val_accuracy: 0.4449\n",
      "Epoch 5/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 8.8576 - accuracy: 0.4047 - val_loss: 4.8920 - val_accuracy: 0.4331\n",
      "Epoch 6/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 4.0031 - accuracy: 0.4864 - val_loss: 2.9500 - val_accuracy: 0.4921\n",
      "Epoch 7/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 2.7904 - accuracy: 0.5097 - val_loss: 2.2428 - val_accuracy: 0.4449\n",
      "Epoch 8/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 2.0542 - accuracy: 0.4961 - val_loss: 1.8357 - val_accuracy: 0.4449\n",
      "Epoch 9/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.7597 - accuracy: 0.5078 - val_loss: 1.5516 - val_accuracy: 0.5039\n",
      "Epoch 10/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.5669 - accuracy: 0.5331 - val_loss: 1.3792 - val_accuracy: 0.5315\n",
      "Epoch 11/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.4093 - accuracy: 0.5370 - val_loss: 1.2687 - val_accuracy: 0.4921\n",
      "Epoch 12/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.3127 - accuracy: 0.5467 - val_loss: 1.1923 - val_accuracy: 0.5197\n",
      "Epoch 13/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1979 - accuracy: 0.5603 - val_loss: 1.0534 - val_accuracy: 0.5394\n",
      "Epoch 14/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0889 - accuracy: 0.5720 - val_loss: 0.9153 - val_accuracy: 0.6063\n",
      "Epoch 15/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9536 - accuracy: 0.5700 - val_loss: 0.8051 - val_accuracy: 0.6063\n",
      "Epoch 16/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8941 - accuracy: 0.5778 - val_loss: 0.7680 - val_accuracy: 0.6378\n",
      "Epoch 17/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8634 - accuracy: 0.5759 - val_loss: 0.7475 - val_accuracy: 0.6220\n",
      "Epoch 18/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8544 - accuracy: 0.5856 - val_loss: 0.7455 - val_accuracy: 0.6220\n",
      "Epoch 19/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8287 - accuracy: 0.6031 - val_loss: 0.7408 - val_accuracy: 0.6063\n",
      "Epoch 20/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7984 - accuracy: 0.6187 - val_loss: 0.7155 - val_accuracy: 0.6378\n",
      "Epoch 21/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8236 - accuracy: 0.6012 - val_loss: 0.7575 - val_accuracy: 0.5906\n",
      "Epoch 22/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7917 - accuracy: 0.5856 - val_loss: 0.7341 - val_accuracy: 0.6063\n",
      "Epoch 23/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7546 - accuracy: 0.6109 - val_loss: 0.7250 - val_accuracy: 0.6024\n",
      "Epoch 24/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7589 - accuracy: 0.6167 - val_loss: 0.7038 - val_accuracy: 0.5866\n",
      "Epoch 25/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7411 - accuracy: 0.6051 - val_loss: 0.6885 - val_accuracy: 0.6142\n",
      "Epoch 26/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7278 - accuracy: 0.6323 - val_loss: 0.6886 - val_accuracy: 0.6181\n",
      "Epoch 27/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7211 - accuracy: 0.6304 - val_loss: 0.6867 - val_accuracy: 0.6220\n",
      "Epoch 28/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7175 - accuracy: 0.6206 - val_loss: 0.7702 - val_accuracy: 0.6457\n",
      "Epoch 29/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7326 - accuracy: 0.6245 - val_loss: 0.7021 - val_accuracy: 0.6102\n",
      "Epoch 30/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7145 - accuracy: 0.6440 - val_loss: 0.6766 - val_accuracy: 0.6260\n",
      "Epoch 31/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7066 - accuracy: 0.6304 - val_loss: 0.6757 - val_accuracy: 0.6260\n",
      "Epoch 32/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7172 - accuracy: 0.6342 - val_loss: 0.7103 - val_accuracy: 0.6142\n",
      "Epoch 33/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7035 - accuracy: 0.6226 - val_loss: 0.6790 - val_accuracy: 0.6299\n",
      "Epoch 34/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6965 - accuracy: 0.6284 - val_loss: 0.6827 - val_accuracy: 0.6299\n",
      "Epoch 35/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6791 - accuracy: 0.6342 - val_loss: 0.7011 - val_accuracy: 0.6024\n",
      "Epoch 36/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7005 - accuracy: 0.6459 - val_loss: 0.7039 - val_accuracy: 0.6024\n",
      "Epoch 37/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.7001 - accuracy: 0.6304 - val_loss: 0.6818 - val_accuracy: 0.6378\n",
      "Epoch 38/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6805 - accuracy: 0.6440 - val_loss: 0.6660 - val_accuracy: 0.6535\n",
      "Epoch 39/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6679 - accuracy: 0.6304 - val_loss: 0.6796 - val_accuracy: 0.6496\n",
      "Epoch 40/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6829 - accuracy: 0.6284 - val_loss: 0.7004 - val_accuracy: 0.6142\n",
      "Epoch 41/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6997 - accuracy: 0.6420 - val_loss: 0.6631 - val_accuracy: 0.6496\n",
      "Epoch 42/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6693 - accuracy: 0.6459 - val_loss: 0.6595 - val_accuracy: 0.6535\n",
      "Epoch 43/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7084 - accuracy: 0.6089 - val_loss: 0.6564 - val_accuracy: 0.6417\n",
      "Epoch 44/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6651 - accuracy: 0.6304 - val_loss: 0.6492 - val_accuracy: 0.6299\n",
      "Epoch 45/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6580 - accuracy: 0.6323 - val_loss: 0.6643 - val_accuracy: 0.6496\n",
      "Epoch 46/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6591 - accuracy: 0.6595 - val_loss: 0.6773 - val_accuracy: 0.6220\n",
      "Epoch 47/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.6498 - val_loss: 0.6609 - val_accuracy: 0.6457\n",
      "Epoch 48/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6586 - accuracy: 0.6634 - val_loss: 0.6683 - val_accuracy: 0.6417\n",
      "Epoch 49/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6650 - accuracy: 0.6440 - val_loss: 0.6584 - val_accuracy: 0.6457\n",
      "Epoch 50/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6449 - accuracy: 0.6440 - val_loss: 0.6592 - val_accuracy: 0.6417\n",
      "Epoch 51/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6464 - accuracy: 0.6401 - val_loss: 0.6457 - val_accuracy: 0.6457\n",
      "Epoch 52/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6653 - accuracy: 0.6304 - val_loss: 0.6498 - val_accuracy: 0.6575\n",
      "Epoch 53/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6449 - accuracy: 0.6381 - val_loss: 0.6724 - val_accuracy: 0.6299\n",
      "Epoch 54/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6611 - accuracy: 0.6440 - val_loss: 0.6695 - val_accuracy: 0.6181\n",
      "Epoch 55/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6579 - accuracy: 0.6362 - val_loss: 0.6479 - val_accuracy: 0.6614\n",
      "Epoch 56/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7422 - accuracy: 0.6206 - val_loss: 0.7348 - val_accuracy: 0.6024\n",
      "Epoch 57/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6461 - accuracy: 0.6440 - val_loss: 0.6683 - val_accuracy: 0.6457\n",
      "Epoch 58/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6416 - accuracy: 0.6498 - val_loss: 0.6433 - val_accuracy: 0.6575\n",
      "Epoch 59/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6519 - accuracy: 0.6576 - val_loss: 0.6464 - val_accuracy: 0.6535\n",
      "Epoch 60/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6310 - accuracy: 0.6673 - val_loss: 0.6545 - val_accuracy: 0.6496\n",
      "Epoch 61/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6682 - accuracy: 0.6498 - val_loss: 0.6602 - val_accuracy: 0.6575\n",
      "Epoch 62/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6486 - accuracy: 0.6693 - val_loss: 0.6382 - val_accuracy: 0.6575\n",
      "Epoch 63/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6359 - accuracy: 0.6537 - val_loss: 0.6518 - val_accuracy: 0.6535\n",
      "Epoch 64/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6472 - accuracy: 0.6459 - val_loss: 0.6425 - val_accuracy: 0.6575\n",
      "Epoch 65/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7165 - accuracy: 0.6459 - val_loss: 0.6270 - val_accuracy: 0.6693\n",
      "Epoch 66/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6566 - accuracy: 0.6518 - val_loss: 0.6397 - val_accuracy: 0.6772\n",
      "Epoch 67/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6261 - accuracy: 0.6615 - val_loss: 0.6450 - val_accuracy: 0.6575\n",
      "Epoch 68/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6793 - accuracy: 0.6245 - val_loss: 0.6509 - val_accuracy: 0.6693\n",
      "Epoch 69/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6219 - accuracy: 0.6518 - val_loss: 0.6279 - val_accuracy: 0.6732\n",
      "Epoch 70/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6115 - accuracy: 0.6673 - val_loss: 0.6690 - val_accuracy: 0.6417\n",
      "Epoch 71/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6480 - accuracy: 0.6576 - val_loss: 0.6530 - val_accuracy: 0.6654\n",
      "Epoch 72/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6269 - accuracy: 0.6518 - val_loss: 0.6703 - val_accuracy: 0.6654\n",
      "Epoch 73/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6349 - accuracy: 0.6518 - val_loss: 0.6311 - val_accuracy: 0.6654\n",
      "Epoch 74/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6140 - accuracy: 0.6790 - val_loss: 0.6515 - val_accuracy: 0.6693\n",
      "Epoch 75/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6354 - accuracy: 0.6518 - val_loss: 0.6294 - val_accuracy: 0.6772\n",
      "Epoch 76/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6255 - accuracy: 0.6615 - val_loss: 0.6291 - val_accuracy: 0.6654\n",
      "Epoch 77/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6131 - accuracy: 0.6926 - val_loss: 0.6168 - val_accuracy: 0.6929\n",
      "Epoch 78/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6247 - accuracy: 0.6556 - val_loss: 0.6179 - val_accuracy: 0.6969\n",
      "Epoch 79/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6106 - accuracy: 0.6498 - val_loss: 0.6366 - val_accuracy: 0.6732\n",
      "Epoch 80/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6416 - accuracy: 0.6323 - val_loss: 0.6834 - val_accuracy: 0.6496\n",
      "Epoch 81/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6139 - accuracy: 0.6518 - val_loss: 0.6382 - val_accuracy: 0.6890\n",
      "Epoch 82/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6263 - accuracy: 0.6770 - val_loss: 0.6366 - val_accuracy: 0.6772\n",
      "Epoch 83/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6052 - accuracy: 0.6790 - val_loss: 0.7193 - val_accuracy: 0.6339\n",
      "Epoch 84/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6340 - accuracy: 0.6693 - val_loss: 0.6344 - val_accuracy: 0.7047\n",
      "Epoch 85/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6088 - accuracy: 0.6595 - val_loss: 0.6354 - val_accuracy: 0.6929\n",
      "Epoch 86/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6104 - accuracy: 0.6848 - val_loss: 0.6435 - val_accuracy: 0.6732\n",
      "Epoch 87/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6046 - accuracy: 0.6693 - val_loss: 0.6252 - val_accuracy: 0.7008\n",
      "Epoch 88/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6240 - accuracy: 0.6615 - val_loss: 0.6288 - val_accuracy: 0.6850\n",
      "Epoch 89/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5981 - accuracy: 0.6712 - val_loss: 0.6438 - val_accuracy: 0.6614\n",
      "Epoch 90/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6030 - accuracy: 0.6809 - val_loss: 0.6678 - val_accuracy: 0.6693\n",
      "Epoch 91/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6086 - accuracy: 0.6634 - val_loss: 0.6308 - val_accuracy: 0.6732\n",
      "Epoch 92/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5983 - accuracy: 0.6848 - val_loss: 0.6477 - val_accuracy: 0.6575\n",
      "Epoch 93/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6009 - accuracy: 0.6634 - val_loss: 0.6319 - val_accuracy: 0.7047\n",
      "Epoch 94/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6094 - accuracy: 0.6420 - val_loss: 0.6406 - val_accuracy: 0.6772\n",
      "Epoch 95/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6124 - accuracy: 0.6693 - val_loss: 0.6165 - val_accuracy: 0.6969\n",
      "Epoch 96/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6085 - accuracy: 0.6634 - val_loss: 0.6064 - val_accuracy: 0.6969\n",
      "Epoch 97/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6391 - accuracy: 0.6498 - val_loss: 0.6019 - val_accuracy: 0.7087\n",
      "Epoch 98/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5968 - accuracy: 0.6634 - val_loss: 0.6142 - val_accuracy: 0.6811\n",
      "Epoch 99/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6005 - accuracy: 0.6790 - val_loss: 0.6094 - val_accuracy: 0.7047\n",
      "Epoch 100/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6066 - accuracy: 0.6615 - val_loss: 0.6155 - val_accuracy: 0.7205\n",
      "Epoch 101/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5955 - accuracy: 0.6790 - val_loss: 0.6228 - val_accuracy: 0.6850\n",
      "Epoch 102/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5966 - accuracy: 0.6848 - val_loss: 0.6200 - val_accuracy: 0.6850\n",
      "Epoch 103/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6010 - accuracy: 0.6654 - val_loss: 0.6188 - val_accuracy: 0.7047\n",
      "Epoch 104/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5881 - accuracy: 0.6829 - val_loss: 0.6063 - val_accuracy: 0.7008\n",
      "Epoch 105/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5941 - accuracy: 0.6829 - val_loss: 0.6580 - val_accuracy: 0.6929\n",
      "Epoch 106/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6336 - accuracy: 0.6693 - val_loss: 0.6564 - val_accuracy: 0.6732\n",
      "Epoch 107/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5937 - accuracy: 0.6732 - val_loss: 0.5980 - val_accuracy: 0.7047\n",
      "Epoch 108/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5865 - accuracy: 0.6907 - val_loss: 0.6013 - val_accuracy: 0.7047\n",
      "Epoch 109/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6097 - accuracy: 0.7004 - val_loss: 0.6515 - val_accuracy: 0.6693\n",
      "Epoch 110/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6035 - accuracy: 0.7043 - val_loss: 0.6125 - val_accuracy: 0.7165\n",
      "Epoch 111/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5975 - accuracy: 0.6770 - val_loss: 0.6241 - val_accuracy: 0.7165\n",
      "Epoch 112/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5764 - accuracy: 0.7023 - val_loss: 0.6333 - val_accuracy: 0.6772\n",
      "Epoch 113/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6123 - accuracy: 0.6595 - val_loss: 0.6109 - val_accuracy: 0.7008\n",
      "Epoch 114/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5719 - accuracy: 0.6887 - val_loss: 0.6187 - val_accuracy: 0.6929\n",
      "Epoch 115/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5992 - accuracy: 0.7062 - val_loss: 0.6404 - val_accuracy: 0.6654\n",
      "Epoch 116/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5926 - accuracy: 0.6946 - val_loss: 0.6293 - val_accuracy: 0.6969\n",
      "Epoch 117/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5809 - accuracy: 0.6848 - val_loss: 0.6123 - val_accuracy: 0.7323\n",
      "Epoch 118/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5757 - accuracy: 0.6829 - val_loss: 0.5974 - val_accuracy: 0.7165\n",
      "Epoch 119/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6058 - accuracy: 0.6693 - val_loss: 0.7141 - val_accuracy: 0.6299\n",
      "Epoch 120/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5893 - accuracy: 0.6809 - val_loss: 0.6069 - val_accuracy: 0.6929\n",
      "Epoch 121/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6077 - accuracy: 0.7004 - val_loss: 0.6102 - val_accuracy: 0.6929\n",
      "Epoch 122/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5986 - accuracy: 0.6751 - val_loss: 0.6127 - val_accuracy: 0.7205\n",
      "Epoch 123/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5677 - accuracy: 0.6984 - val_loss: 0.6129 - val_accuracy: 0.7165\n",
      "Epoch 124/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5917 - accuracy: 0.6848 - val_loss: 0.5914 - val_accuracy: 0.7205\n",
      "Epoch 125/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5694 - accuracy: 0.6907 - val_loss: 0.6247 - val_accuracy: 0.6969\n",
      "Epoch 126/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5717 - accuracy: 0.6868 - val_loss: 0.6478 - val_accuracy: 0.6929\n",
      "Epoch 127/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6007 - accuracy: 0.6673 - val_loss: 0.5987 - val_accuracy: 0.7205\n",
      "Epoch 128/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5626 - accuracy: 0.6926 - val_loss: 0.6035 - val_accuracy: 0.7126\n",
      "Epoch 129/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5834 - accuracy: 0.6926 - val_loss: 0.6354 - val_accuracy: 0.6811\n",
      "Epoch 130/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5794 - accuracy: 0.6965 - val_loss: 0.6081 - val_accuracy: 0.6890\n",
      "Epoch 131/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5751 - accuracy: 0.7004 - val_loss: 0.6187 - val_accuracy: 0.6969\n",
      "Epoch 132/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5738 - accuracy: 0.7198 - val_loss: 0.5941 - val_accuracy: 0.6969\n",
      "Epoch 133/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5786 - accuracy: 0.7101 - val_loss: 0.6571 - val_accuracy: 0.6654\n",
      "Epoch 134/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5889 - accuracy: 0.6809 - val_loss: 0.6122 - val_accuracy: 0.7126\n",
      "Epoch 135/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5582 - accuracy: 0.7062 - val_loss: 0.6190 - val_accuracy: 0.7205\n",
      "Epoch 136/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5630 - accuracy: 0.7218 - val_loss: 0.6055 - val_accuracy: 0.6969\n",
      "Epoch 137/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5639 - accuracy: 0.7004 - val_loss: 0.6084 - val_accuracy: 0.7126\n",
      "Epoch 138/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5728 - accuracy: 0.7315 - val_loss: 0.6242 - val_accuracy: 0.6969\n",
      "Epoch 139/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5710 - accuracy: 0.7101 - val_loss: 0.6031 - val_accuracy: 0.7047\n",
      "Epoch 140/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5605 - accuracy: 0.7043 - val_loss: 0.6144 - val_accuracy: 0.7244\n",
      "Epoch 141/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5953 - accuracy: 0.6926 - val_loss: 0.6226 - val_accuracy: 0.6772\n",
      "Epoch 142/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5666 - accuracy: 0.6887 - val_loss: 0.6052 - val_accuracy: 0.7165\n",
      "Epoch 143/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5537 - accuracy: 0.7160 - val_loss: 0.6046 - val_accuracy: 0.7205\n",
      "Epoch 144/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5634 - accuracy: 0.7160 - val_loss: 0.5979 - val_accuracy: 0.7047\n",
      "Epoch 145/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5609 - accuracy: 0.7140 - val_loss: 0.6407 - val_accuracy: 0.6732\n",
      "Epoch 146/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.5584 - accuracy: 0.7257 - val_loss: 0.5997 - val_accuracy: 0.6890\n",
      "Epoch 147/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5626 - accuracy: 0.7101 - val_loss: 0.5956 - val_accuracy: 0.7047\n",
      "Epoch 148/150\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.6333 - accuracy: 0.6926 - val_loss: 0.7680 - val_accuracy: 0.6024\n",
      "Epoch 149/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5702 - accuracy: 0.7179 - val_loss: 0.6089 - val_accuracy: 0.6654\n",
      "Epoch 150/150\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5619 - accuracy: 0.7160 - val_loss: 0.6054 - val_accuracy: 0.7205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x257426090c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos \n",
    "dataset=np.loadtxt('D:/Ivan/Tesis/EigenfacesPython/3.2.+Dise√±oRedNeuronal/Datasets/pima-indians-diabetes.csv',delimiter=',')\n",
    "print(dataset.shape)\n",
    "X=dataset[:,0:8]\n",
    "y=dataset[:,8]\n",
    "\n",
    "#Definir el modelo\n",
    "model= Sequential()\n",
    "model.add(Dense(12,input_dim=8,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Ajuste del modelo\n",
    "model.fit(X,y, validation_split=0.33, epochs=150,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "Epoch 1/150\n",
      "33/33 [==============================] - 1s 7ms/step - loss: 53.4254 - accuracy: 0.6479 - val_loss: 29.2008 - val_accuracy: 0.5787\n",
      "Epoch 2/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 12.9250 - accuracy: 0.4786 - val_loss: 10.0876 - val_accuracy: 0.4173\n",
      "Epoch 3/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 7.9452 - accuracy: 0.4728 - val_loss: 6.8127 - val_accuracy: 0.4685\n",
      "Epoch 4/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 5.1750 - accuracy: 0.4961 - val_loss: 4.6313 - val_accuracy: 0.5000\n",
      "Epoch 5/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 3.4246 - accuracy: 0.5253 - val_loss: 3.3026 - val_accuracy: 0.5236\n",
      "Epoch 6/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.4955 - accuracy: 0.5447 - val_loss: 2.7388 - val_accuracy: 0.5709\n",
      "Epoch 7/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.3636 - accuracy: 0.5603 - val_loss: 2.5555 - val_accuracy: 0.5866\n",
      "Epoch 8/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.1403 - accuracy: 0.5914 - val_loss: 2.0640 - val_accuracy: 0.5630\n",
      "Epoch 9/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.8692 - accuracy: 0.6051 - val_loss: 1.8557 - val_accuracy: 0.5551\n",
      "Epoch 10/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.7126 - accuracy: 0.6148 - val_loss: 1.7410 - val_accuracy: 0.5787\n",
      "Epoch 11/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.6826 - accuracy: 0.6284 - val_loss: 1.7949 - val_accuracy: 0.5906\n",
      "Epoch 12/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.9451 - accuracy: 0.5681 - val_loss: 1.9488 - val_accuracy: 0.6024\n",
      "Epoch 13/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.6028 - accuracy: 0.6167 - val_loss: 1.8512 - val_accuracy: 0.5472\n",
      "Epoch 14/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5873 - accuracy: 0.6128 - val_loss: 1.7806 - val_accuracy: 0.6024\n",
      "Epoch 15/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4661 - accuracy: 0.6187 - val_loss: 1.4226 - val_accuracy: 0.5748\n",
      "Epoch 16/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.4148 - accuracy: 0.6031 - val_loss: 1.4019 - val_accuracy: 0.5748\n",
      "Epoch 17/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3558 - accuracy: 0.6109 - val_loss: 1.4232 - val_accuracy: 0.5669\n",
      "Epoch 18/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4086 - accuracy: 0.6148 - val_loss: 1.7287 - val_accuracy: 0.5906\n",
      "Epoch 19/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2905 - accuracy: 0.6070 - val_loss: 1.2653 - val_accuracy: 0.5827\n",
      "Epoch 20/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.2369 - accuracy: 0.6284 - val_loss: 1.1381 - val_accuracy: 0.6063\n",
      "Epoch 21/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2053 - accuracy: 0.6148 - val_loss: 1.1657 - val_accuracy: 0.6024\n",
      "Epoch 22/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.0982 - accuracy: 0.6362 - val_loss: 1.0358 - val_accuracy: 0.6142\n",
      "Epoch 23/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.0439 - accuracy: 0.6537 - val_loss: 1.0549 - val_accuracy: 0.6220\n",
      "Epoch 24/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.9682 - accuracy: 0.6498 - val_loss: 0.9747 - val_accuracy: 0.6339\n",
      "Epoch 25/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.9836 - accuracy: 0.6479 - val_loss: 1.0804 - val_accuracy: 0.5984\n",
      "Epoch 26/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.0174 - accuracy: 0.6342 - val_loss: 1.4204 - val_accuracy: 0.5000\n",
      "Epoch 27/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1566 - accuracy: 0.6323 - val_loss: 1.2015 - val_accuracy: 0.5118\n",
      "Epoch 28/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.0763 - accuracy: 0.5934 - val_loss: 1.0223 - val_accuracy: 0.6142\n",
      "Epoch 29/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.8803 - accuracy: 0.6556 - val_loss: 1.0355 - val_accuracy: 0.6181\n",
      "Epoch 30/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.9451 - accuracy: 0.6167 - val_loss: 0.8873 - val_accuracy: 0.6614\n",
      "Epoch 31/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.8686 - accuracy: 0.6381 - val_loss: 0.9381 - val_accuracy: 0.6299\n",
      "Epoch 32/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8882 - accuracy: 0.6440 - val_loss: 0.8337 - val_accuracy: 0.6299\n",
      "Epoch 33/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.8152 - accuracy: 0.6595 - val_loss: 0.9278 - val_accuracy: 0.6260\n",
      "Epoch 34/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.8147 - accuracy: 0.6479 - val_loss: 0.8253 - val_accuracy: 0.6220\n",
      "Epoch 35/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.8000 - accuracy: 0.6615 - val_loss: 0.7818 - val_accuracy: 0.6496\n",
      "Epoch 36/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.8225 - accuracy: 0.6323 - val_loss: 0.9571 - val_accuracy: 0.5945\n",
      "Epoch 37/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8911 - accuracy: 0.6323 - val_loss: 0.8488 - val_accuracy: 0.6220\n",
      "Epoch 38/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7756 - accuracy: 0.6420 - val_loss: 0.7641 - val_accuracy: 0.6614\n",
      "Epoch 39/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7620 - accuracy: 0.6673 - val_loss: 0.7522 - val_accuracy: 0.6457\n",
      "Epoch 40/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7674 - accuracy: 0.6537 - val_loss: 0.7723 - val_accuracy: 0.6181\n",
      "Epoch 41/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7681 - accuracy: 0.6654 - val_loss: 0.7873 - val_accuracy: 0.6417\n",
      "Epoch 42/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.8828 - accuracy: 0.6498 - val_loss: 0.7382 - val_accuracy: 0.6496\n",
      "Epoch 43/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7247 - accuracy: 0.6498 - val_loss: 0.8123 - val_accuracy: 0.6457\n",
      "Epoch 44/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7295 - accuracy: 0.6595 - val_loss: 0.7483 - val_accuracy: 0.6299\n",
      "Epoch 45/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.6693 - val_loss: 0.8160 - val_accuracy: 0.6339\n",
      "Epoch 46/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7417 - accuracy: 0.6712 - val_loss: 0.7077 - val_accuracy: 0.6378\n",
      "Epoch 47/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8302 - accuracy: 0.6401 - val_loss: 0.7228 - val_accuracy: 0.6299\n",
      "Epoch 48/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6711 - accuracy: 0.6556 - val_loss: 0.7160 - val_accuracy: 0.6811\n",
      "Epoch 49/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6990 - accuracy: 0.6809 - val_loss: 0.6936 - val_accuracy: 0.6535\n",
      "Epoch 50/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8220 - accuracy: 0.6498 - val_loss: 0.7298 - val_accuracy: 0.6378\n",
      "Epoch 51/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.6498 - val_loss: 0.7373 - val_accuracy: 0.6654\n",
      "Epoch 52/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6997 - accuracy: 0.6440 - val_loss: 0.8980 - val_accuracy: 0.5433\n",
      "Epoch 53/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7493 - accuracy: 0.6401 - val_loss: 0.7505 - val_accuracy: 0.6378\n",
      "Epoch 54/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7641 - accuracy: 0.6498 - val_loss: 0.6971 - val_accuracy: 0.6732\n",
      "Epoch 55/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6483 - accuracy: 0.6615 - val_loss: 0.7388 - val_accuracy: 0.6339\n",
      "Epoch 56/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7864 - accuracy: 0.6459 - val_loss: 0.8426 - val_accuracy: 0.5472\n",
      "Epoch 57/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8624 - accuracy: 0.6187 - val_loss: 0.6633 - val_accuracy: 0.6929\n",
      "Epoch 58/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7103 - accuracy: 0.6498 - val_loss: 0.6557 - val_accuracy: 0.6654\n",
      "Epoch 59/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6359 - accuracy: 0.6887 - val_loss: 0.6519 - val_accuracy: 0.6693\n",
      "Epoch 60/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6541 - accuracy: 0.6868 - val_loss: 0.6467 - val_accuracy: 0.6811\n",
      "Epoch 61/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.6751 - val_loss: 0.6527 - val_accuracy: 0.6969\n",
      "Epoch 62/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6406 - accuracy: 0.6712 - val_loss: 0.7182 - val_accuracy: 0.6378\n",
      "Epoch 63/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6516 - accuracy: 0.6634 - val_loss: 0.8663 - val_accuracy: 0.6142\n",
      "Epoch 64/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.6751 - val_loss: 0.8477 - val_accuracy: 0.6220\n",
      "Epoch 65/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8252 - accuracy: 0.6518 - val_loss: 0.7094 - val_accuracy: 0.6535\n",
      "Epoch 66/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6510 - accuracy: 0.6693 - val_loss: 0.6657 - val_accuracy: 0.6693\n",
      "Epoch 67/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6368 - accuracy: 0.6907 - val_loss: 0.6417 - val_accuracy: 0.6732\n",
      "Epoch 68/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6160 - accuracy: 0.6673 - val_loss: 0.6431 - val_accuracy: 0.6614\n",
      "Epoch 69/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5997 - accuracy: 0.6926 - val_loss: 0.6809 - val_accuracy: 0.6614\n",
      "Epoch 70/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5923 - accuracy: 0.7043 - val_loss: 0.6756 - val_accuracy: 0.6260\n",
      "Epoch 71/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.6693 - val_loss: 0.6762 - val_accuracy: 0.6339\n",
      "Epoch 72/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.6887 - val_loss: 0.9600 - val_accuracy: 0.5118\n",
      "Epoch 73/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7222 - accuracy: 0.6459 - val_loss: 0.6446 - val_accuracy: 0.6850\n",
      "Epoch 74/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6079 - accuracy: 0.7121 - val_loss: 0.6273 - val_accuracy: 0.6890\n",
      "Epoch 75/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6078 - accuracy: 0.6887 - val_loss: 0.6240 - val_accuracy: 0.7126\n",
      "Epoch 76/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6085 - accuracy: 0.6829 - val_loss: 0.6212 - val_accuracy: 0.7126\n",
      "Epoch 77/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6272 - accuracy: 0.6537 - val_loss: 0.6337 - val_accuracy: 0.7244\n",
      "Epoch 78/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6021 - accuracy: 0.6984 - val_loss: 0.6239 - val_accuracy: 0.7047\n",
      "Epoch 79/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5905 - accuracy: 0.7004 - val_loss: 0.6665 - val_accuracy: 0.6457\n",
      "Epoch 80/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5895 - accuracy: 0.6848 - val_loss: 0.6612 - val_accuracy: 0.6457\n",
      "Epoch 81/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5954 - accuracy: 0.7121 - val_loss: 0.7098 - val_accuracy: 0.6339\n",
      "Epoch 82/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6208 - accuracy: 0.6732 - val_loss: 0.6811 - val_accuracy: 0.6457\n",
      "Epoch 83/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6121 - accuracy: 0.6926 - val_loss: 0.6362 - val_accuracy: 0.6614\n",
      "Epoch 84/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6343 - accuracy: 0.6693 - val_loss: 0.7027 - val_accuracy: 0.6378\n",
      "Epoch 85/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7484 - accuracy: 0.6673 - val_loss: 0.7267 - val_accuracy: 0.6339\n",
      "Epoch 86/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6150 - accuracy: 0.6926 - val_loss: 0.6308 - val_accuracy: 0.6693\n",
      "Epoch 87/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5729 - accuracy: 0.7023 - val_loss: 0.6557 - val_accuracy: 0.6732\n",
      "Epoch 88/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5906 - accuracy: 0.6926 - val_loss: 0.6618 - val_accuracy: 0.6575\n",
      "Epoch 89/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.8249 - accuracy: 0.6498 - val_loss: 0.8123 - val_accuracy: 0.6299\n",
      "Epoch 90/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6229 - accuracy: 0.7023 - val_loss: 0.6120 - val_accuracy: 0.7165\n",
      "Epoch 91/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6075 - accuracy: 0.6848 - val_loss: 0.6082 - val_accuracy: 0.7480\n",
      "Epoch 92/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.7023 - val_loss: 0.7499 - val_accuracy: 0.5984\n",
      "Epoch 93/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6306 - accuracy: 0.6712 - val_loss: 0.6112 - val_accuracy: 0.7047\n",
      "Epoch 94/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5769 - accuracy: 0.7121 - val_loss: 0.6065 - val_accuracy: 0.7402\n",
      "Epoch 95/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7527 - accuracy: 0.6401 - val_loss: 0.6119 - val_accuracy: 0.7047\n",
      "Epoch 96/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5564 - accuracy: 0.7198 - val_loss: 0.6859 - val_accuracy: 0.6850\n",
      "Epoch 97/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5879 - accuracy: 0.7140 - val_loss: 0.7761 - val_accuracy: 0.5827\n",
      "Epoch 98/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6200 - accuracy: 0.7004 - val_loss: 0.7821 - val_accuracy: 0.6339\n",
      "Epoch 99/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7235 - accuracy: 0.6712 - val_loss: 0.6409 - val_accuracy: 0.6929\n",
      "Epoch 100/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5628 - accuracy: 0.7335 - val_loss: 0.6075 - val_accuracy: 0.7323\n",
      "Epoch 101/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5612 - accuracy: 0.7315 - val_loss: 0.6002 - val_accuracy: 0.7480\n",
      "Epoch 102/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5934 - accuracy: 0.7023 - val_loss: 0.6355 - val_accuracy: 0.6732\n",
      "Epoch 103/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6178 - accuracy: 0.7023 - val_loss: 0.7727 - val_accuracy: 0.6299\n",
      "Epoch 104/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6841 - accuracy: 0.6634 - val_loss: 0.8619 - val_accuracy: 0.5236\n",
      "Epoch 105/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7223 - accuracy: 0.6693 - val_loss: 0.8330 - val_accuracy: 0.6339\n",
      "Epoch 106/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6099 - accuracy: 0.7198 - val_loss: 0.7325 - val_accuracy: 0.6339\n",
      "Epoch 107/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5827 - accuracy: 0.6887 - val_loss: 0.6514 - val_accuracy: 0.6929\n",
      "Epoch 108/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.6673 - val_loss: 0.8387 - val_accuracy: 0.6457\n",
      "Epoch 109/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7037 - accuracy: 0.6809 - val_loss: 0.9825 - val_accuracy: 0.5118\n",
      "Epoch 110/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7778 - accuracy: 0.6381 - val_loss: 0.7512 - val_accuracy: 0.6299\n",
      "Epoch 111/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6253 - accuracy: 0.6984 - val_loss: 0.6124 - val_accuracy: 0.7087\n",
      "Epoch 112/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6593 - accuracy: 0.7023 - val_loss: 0.6741 - val_accuracy: 0.6575\n",
      "Epoch 113/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7904 - accuracy: 0.6615 - val_loss: 0.8995 - val_accuracy: 0.6575\n",
      "Epoch 114/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5799 - accuracy: 0.7140 - val_loss: 0.6063 - val_accuracy: 0.7283\n",
      "Epoch 115/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5767 - accuracy: 0.7296 - val_loss: 0.9128 - val_accuracy: 0.5276\n",
      "Epoch 116/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6214 - accuracy: 0.6946 - val_loss: 0.8496 - val_accuracy: 0.6378\n",
      "Epoch 117/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7423 - accuracy: 0.6615 - val_loss: 0.6037 - val_accuracy: 0.7362\n",
      "Epoch 118/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5754 - accuracy: 0.7315 - val_loss: 0.6590 - val_accuracy: 0.6772\n",
      "Epoch 119/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5814 - accuracy: 0.7237 - val_loss: 0.7727 - val_accuracy: 0.6496\n",
      "Epoch 120/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7354 - val_loss: 0.6268 - val_accuracy: 0.6929\n",
      "Epoch 121/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5430 - accuracy: 0.7335 - val_loss: 0.5987 - val_accuracy: 0.7441\n",
      "Epoch 122/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5300 - accuracy: 0.7490 - val_loss: 0.6301 - val_accuracy: 0.6772\n",
      "Epoch 123/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5670 - accuracy: 0.7257 - val_loss: 0.6009 - val_accuracy: 0.7520\n",
      "Epoch 124/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5718 - accuracy: 0.7296 - val_loss: 0.6306 - val_accuracy: 0.6811\n",
      "Epoch 125/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.7335 - val_loss: 0.6649 - val_accuracy: 0.6614\n",
      "Epoch 126/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.7808 - accuracy: 0.6440 - val_loss: 0.5997 - val_accuracy: 0.7244\n",
      "Epoch 127/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7685 - val_loss: 0.5996 - val_accuracy: 0.7362\n",
      "Epoch 128/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5474 - accuracy: 0.7412 - val_loss: 0.6731 - val_accuracy: 0.6496\n",
      "Epoch 129/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6280 - accuracy: 0.6887 - val_loss: 0.7045 - val_accuracy: 0.6614\n",
      "Epoch 130/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5354 - accuracy: 0.7335 - val_loss: 0.7477 - val_accuracy: 0.6181\n",
      "Epoch 131/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6612 - accuracy: 0.6984 - val_loss: 0.6084 - val_accuracy: 0.7205\n",
      "Epoch 132/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6192 - accuracy: 0.7160 - val_loss: 0.6714 - val_accuracy: 0.6693\n",
      "Epoch 133/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5615 - accuracy: 0.7393 - val_loss: 0.6095 - val_accuracy: 0.7126\n",
      "Epoch 134/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5803 - accuracy: 0.7160 - val_loss: 0.6312 - val_accuracy: 0.6811\n",
      "Epoch 135/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5743 - accuracy: 0.7276 - val_loss: 0.9493 - val_accuracy: 0.6339\n",
      "Epoch 136/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5650 - accuracy: 0.7529 - val_loss: 0.6065 - val_accuracy: 0.7126\n",
      "Epoch 137/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5401 - accuracy: 0.7588 - val_loss: 0.7128 - val_accuracy: 0.6417\n",
      "Epoch 138/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6322 - accuracy: 0.7082 - val_loss: 0.6446 - val_accuracy: 0.6811\n",
      "Epoch 139/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5996 - accuracy: 0.7023 - val_loss: 0.7316 - val_accuracy: 0.6457\n",
      "Epoch 140/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.7412 - val_loss: 0.7319 - val_accuracy: 0.6378\n",
      "Epoch 141/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5155 - accuracy: 0.7549 - val_loss: 0.6031 - val_accuracy: 0.7165\n",
      "Epoch 142/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5863 - accuracy: 0.7160 - val_loss: 0.6717 - val_accuracy: 0.6614\n",
      "Epoch 143/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7087 - accuracy: 0.6595 - val_loss: 0.7231 - val_accuracy: 0.6535\n",
      "Epoch 144/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5492 - accuracy: 0.7315 - val_loss: 0.6429 - val_accuracy: 0.6654\n",
      "Epoch 145/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7588 - val_loss: 0.6020 - val_accuracy: 0.7165\n",
      "Epoch 146/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6270 - accuracy: 0.6887 - val_loss: 0.6086 - val_accuracy: 0.6969\n",
      "Epoch 147/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6243 - accuracy: 0.7296 - val_loss: 0.6020 - val_accuracy: 0.7402\n",
      "Epoch 148/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5727 - accuracy: 0.7257 - val_loss: 0.6051 - val_accuracy: 0.7323\n",
      "Epoch 149/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5978 - accuracy: 0.7140 - val_loss: 0.6653 - val_accuracy: 0.6732\n",
      "Epoch 150/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7529 - val_loss: 0.6101 - val_accuracy: 0.7244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c38a70fd30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar datos \n",
    "dataset=np.loadtxt('D:/Ivan/Tesis/EigenfacesPython/3.2.+Dise√±oRedNeuronal/Datasets/pima-indians-diabetes.csv',delimiter=',')\n",
    "print(dataset.shape)\n",
    "X=dataset[:,0:8]\n",
    "y=dataset[:,8]\n",
    "\n",
    "# Split de datos\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.33)\n",
    "\n",
    "#Definir el modelo\n",
    "model= Sequential()\n",
    "model.add(Dense(12,input_dim=8,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Ajuste del modelo\n",
    "model.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=150,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "accuracy: 67.53%\n",
      "accuracy: 64.94%\n",
      "accuracy: 68.83%\n",
      "accuracy: 63.64%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001B790E6B0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 68.83%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001B78F8A4F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 74.03%\n",
      "accuracy: 58.44%\n",
      "accuracy: 72.73%\n",
      "accuracy: 68.42%\n",
      "accuracy: 67.11%\n",
      "67.45% (+/- 4.22%)\n"
     ]
    }
   ],
   "source": [
    "# Validaci√≥n Cruzada\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos \n",
    "dataset=np.loadtxt('D:/Ivan/Tesis/EigenfacesPython/3.2.+Dise√±oRedNeuronal/Datasets/pima-indians-diabetes.csv',delimiter=',')\n",
    "print(dataset.shape)\n",
    "X=dataset[:,0:8]\n",
    "y=dataset[:,8]\n",
    "\n",
    "# Definir un 10-fold cross validation test\n",
    "kfold= StratifiedKFold(n_splits=10, shuffle=True)\n",
    "cvscores= []\n",
    "for train , test in kfold.split(X,y):\n",
    "    #Definir el modelo\n",
    "    model= Sequential()\n",
    "    model.add(Dense(12,input_dim=8,activation='relu'))\n",
    "    model.add(Dense(8,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    # Compilar el modelo\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    # Ajuste del modelo\n",
    "    model.fit(X[train], y[train], epochs=150,batch_size=16, verbose=0)\n",
    "    # Evaluar del modelo\n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\"% (model.metrics_names[1],scores[1]*100))\n",
    "    cvscores.append(scores[1]*100)\n",
    "# Imprimir resultado global\n",
    "print (\"%.2f%% (+/- %.2f%%)\" %(np.mean(cvscores), np.std(cvscores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BBY2021\\AppData\\Local\\Temp\\ipykernel_10056\\3751286167.py:26: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model= KerasClassifier(build_fn= create_model, epochs=150, batch_size=16, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6550239205360413\n"
     ]
    }
   ],
   "source": [
    "# Keras-Sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Crear modelo\n",
    "def create_model():\n",
    "    model= Sequential()\n",
    "    model.add(Dense(12,input_dim=8,activation='relu'))\n",
    "    model.add(Dense(8,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    # Compilar el modelo\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Cargar datos \n",
    "dataset=np.loadtxt('D:/Ivan/Tesis/EigenfacesPython/3.2.+Dise√±oRedNeuronal/Datasets/pima-indians-diabetes.csv',delimiter=',')\n",
    "print(dataset.shape)\n",
    "X=dataset[:,0:8]\n",
    "y=dataset[:,8]\n",
    "\n",
    "# Crear modelo\n",
    "model= KerasClassifier(build_fn= create_model, epochs=150, batch_size=16, verbose=0)\n",
    "kfold= StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results= cross_val_score(model,X,y,cv=kfold)\n",
    "print(results.mean())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BBY2021\\AppData\\Local\\Temp\\ipykernel_19280\\3467282536.py:25: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model= KerasClassifier(build_fn= create_model, verbose=0)\n",
      "d:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "135 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python\\Lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 248, in fit\n",
      "    return super().fit(x, y, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Python\\Lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 164, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BBY2021\\AppData\\Local\\Temp\\ipykernel_19280\\3467282536.py\", line 14, in create_model\n",
      "    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
      "  File \"d:\\Python\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"d:\\Python\\Lib\\site-packages\\keras\\saving\\legacy\\serialization.py\", line 368, in class_and_config_for_serialized_keras_object\n",
      "    raise ValueError(\n",
      "ValueError: Unknown optimizer: 'resprop'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.64331551        nan 0.71498176        nan 0.70841186\n",
      "        nan 0.70193533        nan 0.72141584        nan 0.72141585\n",
      "        nan 0.70960021        nan 0.74612513        nan 0.74750023\n",
      "        nan 0.62768016        nan 0.72276548        nan 0.69540787\n",
      "        nan 0.7031831         nan 0.72791785        nan 0.72924201\n",
      "        nan 0.68096937        nan 0.74357017        nan 0.73576097\n",
      "        nan 0.64070113        nan 0.69015365        nan 0.67450132\n",
      "        nan 0.66265174        nan 0.7304813         nan 0.71883543\n",
      "        nan 0.71236738        nan 0.72794331        nan 0.73966558]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor: 0.747500 usando {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'resprop'}\n",
      "0.643316 (0.035400) con: {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'resprop'}\n",
      "0.714982 (0.052910) con: {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'resprop'}\n",
      "0.708412 (0.044945) con: {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'resprop'}\n",
      "0.701935 (0.044993) con: {'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'resprop'}\n",
      "0.721416 (0.053068) con: {'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'resprop'}\n",
      "0.721416 (0.031426) con: {'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'resprop'}\n",
      "0.709600 (0.041955) con: {'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'resprop'}\n",
      "0.746125 (0.037843) con: {'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'resprop'}\n",
      "0.747500 (0.041710) con: {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'resprop'}\n",
      "0.627680 (0.034441) con: {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'resprop'}\n",
      "0.722765 (0.043240) con: {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'resprop'}\n",
      "0.695408 (0.039776) con: {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'resprop'}\n",
      "0.703183 (0.033310) con: {'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'resprop'}\n",
      "0.727918 (0.036158) con: {'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'resprop'}\n",
      "0.729242 (0.036706) con: {'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'resprop'}\n",
      "0.680969 (0.042863) con: {'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'resprop'}\n",
      "0.743570 (0.038301) con: {'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'resprop'}\n",
      "0.735761 (0.040999) con: {'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'resprop'}\n",
      "0.640701 (0.031418) con: {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'resprop'}\n",
      "0.690154 (0.025607) con: {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'resprop'}\n",
      "0.674501 (0.017516) con: {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'resprop'}\n",
      "0.662652 (0.044709) con: {'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'resprop'}\n",
      "0.730481 (0.020299) con: {'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'resprop'}\n",
      "0.718835 (0.028721) con: {'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'resprop'}\n",
      "0.712367 (0.040607) con: {'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'resprop'}\n",
      "0.727943 (0.036141) con: {'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
      "nan (nan) con: {'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'resprop'}\n",
      "0.739666 (0.028688) con: {'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Crear modelo\n",
    "def create_model(optimizer='resprop', init='glorot_uniform'):\n",
    "    model= Sequential()\n",
    "    model.add(Dense(12,input_dim=8,kernel_initializer=init,activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer=init,activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init,activation='sigmoid'))\n",
    "    # Compilar el modelo\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Cargar datos \n",
    "dataset=np.loadtxt('D:/Ivan/Tesis/EigenfacesPython/3.2.+Dise√±oRedNeuronal/Datasets/pima-indians-diabetes.csv',delimiter=',')\n",
    "print(dataset.shape)\n",
    "X=dataset[:,0:8]\n",
    "y=dataset[:,8]\n",
    "\n",
    "# Crear modelo\n",
    "## Actualizar el formato para sustituir KerasClassifier ##\n",
    "model= KerasClassifier(build_fn= create_model, verbose=0)\n",
    "\n",
    "#Grid search\n",
    "optimizers = ['resprop','adam']\n",
    "inits = ['glorot_uniform','normal', 'uniform']\n",
    "epochs=[50,100,150]\n",
    "batches=[5,10,20]\n",
    "\n",
    "param_grid=dict(optimizer=optimizers,epochs=epochs,batch_size=batches,init=inits)\n",
    "grid=GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result= grid.fit(X,y)\n",
    "\n",
    "#Resumen de resultados\n",
    "print(\"Mejor: %f usando %s\" % (grid_result.best_score_ , grid_result.best_params_))\n",
    "means=grid_result.cv_results_['mean_test_score']\n",
    "stds=grid_result.cv_results_['std_test_score']\n",
    "params=grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip (means,stds,params):\n",
    "    print(\"%f (%f) con: %r\" % (mean,stdev,param))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorgpu",
   "language": "python",
   "name": "tensorgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
